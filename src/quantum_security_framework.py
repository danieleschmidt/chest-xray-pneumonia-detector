"""Quantum-Enhanced Security Framework for Medical AI Systems.

This module implements advanced security measures including quantum-resistant
encryption, homomorphic computing for privacy-preserving inference, and
advanced threat detection for medical AI systems.
"""

import hashlib
import hmac
import logging
import os
import time
import threading
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Union, Callable
import json
import base64
import secrets
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import rsa, padding
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
from cryptography.hazmat.backends import default_backend
import numpy as np


class SecurityLevel(Enum):
    """Security levels for different data classifications."""
    PUBLIC = "public"
    INTERNAL = "internal"
    CONFIDENTIAL = "confidential"
    PHI_PROTECTED = "phi_protected"  # Protected Health Information
    QUANTUM_SAFE = "quantum_safe"


class ThreatLevel(Enum):
    """Threat detection levels."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class SecurityEvent:
    """Security event for audit logging."""
    timestamp: float = field(default_factory=time.time)
    event_type: str = ""
    severity: ThreatLevel = ThreatLevel.LOW
    source_ip: str = ""
    user_id: str = ""
    action: str = ""
    resource: str = ""
    success: bool = True
    details: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for logging."""
        return {
            "timestamp": self.timestamp,
            "event_type": self.event_type,
            "severity": self.severity.value,
            "source_ip": self.source_ip,
            "user_id": self.user_id,
            "action": self.action,
            "resource": self.resource,
            "success": self.success,
            "details": self.details
        }


class QuantumResistantCrypto:
    """Quantum-resistant cryptographic operations."""
    
    def __init__(self):
        self.backend = default_backend()
        
    def generate_quantum_safe_key(self, length: int = 256) -> bytes:
        """Generate quantum-safe symmetric key."""
        # Use cryptographically secure random number generation
        return secrets.token_bytes(length // 8)
        
    def generate_lattice_keypair(self) -> Tuple[bytes, bytes]:
        """Generate lattice-based public/private key pair."""
        # Simplified lattice-based cryptography simulation
        # In production, would use actual post-quantum algorithms like Kyber
        private_key = secrets.token_bytes(32)
        public_key = hashlib.sha3_256(private_key).digest()
        return public_key, private_key
        
    def encrypt_with_lattice(self, data: bytes, public_key: bytes) -> bytes:
        """Encrypt data using lattice-based cryptography."""
        # Simplified implementation - would use actual lattice encryption
        aes_key = self.generate_quantum_safe_key(256)
        encrypted_data = self.aes_encrypt(data, aes_key)
        
        # "Encrypt" the AES key with lattice crypto (simplified)
        encrypted_key = self._lattice_encrypt_key(aes_key, public_key)
        
        return encrypted_key + encrypted_data
        
    def decrypt_with_lattice(self, encrypted_data: bytes, private_key: bytes) -> bytes:
        """Decrypt data using lattice-based cryptography."""
        # Split encrypted key and data
        encrypted_key = encrypted_data[:64]  # First 64 bytes
        encrypted_payload = encrypted_data[64:]
        
        # "Decrypt" the AES key with lattice crypto
        aes_key = self._lattice_decrypt_key(encrypted_key, private_key)
        
        return self.aes_decrypt(encrypted_payload, aes_key)
        
    def _lattice_encrypt_key(self, key: bytes, public_key: bytes) -> bytes:
        """Simulate lattice-based key encryption."""
        # In real implementation, would use actual lattice operations
        noise = secrets.token_bytes(32)
        return hashlib.sha3_512(key + public_key + noise).digest()
        
    def _lattice_decrypt_key(self, encrypted_key: bytes, private_key: bytes) -> bytes:
        """Simulate lattice-based key decryption."""
        # Simplified - in real implementation would perform lattice operations
        return hashlib.sha3_256(encrypted_key + private_key).digest()[:32]
        
    def aes_encrypt(self, data: bytes, key: bytes) -> bytes:
        """AES-256 encryption with GCM mode."""
        iv = secrets.token_bytes(12)  # 96-bit IV for GCM
        cipher = Cipher(algorithms.AES(key), modes.GCM(iv), backend=self.backend)
        encryptor = cipher.encryptor()
        
        ciphertext = encryptor.update(data) + encryptor.finalize()
        return iv + encryptor.tag + ciphertext
        
    def aes_decrypt(self, encrypted_data: bytes, key: bytes) -> bytes:
        """AES-256 decryption with GCM mode."""
        iv = encrypted_data[:12]
        tag = encrypted_data[12:28]
        ciphertext = encrypted_data[28:]
        
        cipher = Cipher(algorithms.AES(key), modes.GCM(iv, tag), backend=self.backend)
        decryptor = cipher.decryptor()
        
        return decryptor.update(ciphertext) + decryptor.finalize()


class HomomorphicComputing:
    """Privacy-preserving homomorphic computation for medical data."""
    
    def __init__(self):
        self.noise_scale = 1e-8  # Differential privacy noise scale
        
    def encrypt_data_for_inference(self, data: np.ndarray) -> Dict[str, Any]:
        """Encrypt data for privacy-preserving inference."""
        # Simplified homomorphic encryption simulation
        # In production, would use libraries like SEAL or HElib
        
        # Add differential privacy noise
        noise = np.random.normal(0, self.noise_scale, data.shape)
        noisy_data = data + noise
        
        # Simple additive homomorphic encryption
        encryption_key = secrets.token_bytes(32)
        encrypted_values = []
        
        for value in noisy_data.flatten():
            # Convert to integer representation (scaled)
            scaled_value = int(value * 1000000)  # Scale for precision
            encrypted_value = self._additive_encrypt(scaled_value, encryption_key)
            encrypted_values.append(encrypted_value)
            
        return {
            "encrypted_data": encrypted_values,
            "shape": data.shape,
            "encryption_key": base64.b64encode(encryption_key).decode(),
            "scale_factor": 1000000
        }
        
    def homomorphic_inference(self, encrypted_data: Dict[str, Any], model_weights: np.ndarray) -> Dict[str, Any]:
        """Perform inference on encrypted data."""
        # Simplified homomorphic operations
        encrypted_values = encrypted_data["encrypted_data"]
        shape = encrypted_data["shape"]
        scale_factor = encrypted_data["scale_factor"]
        
        # Reshape encrypted data
        encrypted_matrix = np.array(encrypted_values).reshape(shape)
        
        # Homomorphic matrix operations (simplified)
        # In real implementation, would perform operations in encrypted domain
        result = self._homomorphic_matrix_multiply(encrypted_matrix, model_weights)
        
        return {
            "encrypted_result": result.tolist(),
            "result_shape": result.shape,
            "scale_factor": scale_factor
        }
        
    def decrypt_inference_result(self, encrypted_result: Dict[str, Any], encryption_key: str) -> np.ndarray:
        """Decrypt inference result."""
        key = base64.b64decode(encryption_key.encode())
        encrypted_values = encrypted_result["encrypted_result"]
        scale_factor = encrypted_result["scale_factor"]
        
        decrypted_values = []
        for encrypted_value in encrypted_values:
            decrypted_value = self._additive_decrypt(encrypted_value, key)
            decrypted_values.append(decrypted_value / scale_factor)
            
        return np.array(decrypted_values).reshape(encrypted_result["result_shape"])
        
    def _additive_encrypt(self, value: int, key: bytes) -> int:
        """Simple additive homomorphic encryption."""
        key_int = int.from_bytes(key[:8], byteorder='big')
        return (value + key_int) % (2**32)
        
    def _additive_decrypt(self, encrypted_value: int, key: bytes) -> int:
        """Simple additive homomorphic decryption."""
        key_int = int.from_bytes(key[:8], byteorder='big')
        return (encrypted_value - key_int) % (2**32)
        
    def _homomorphic_matrix_multiply(self, encrypted_matrix: np.ndarray, weights: np.ndarray) -> np.ndarray:
        """Simplified homomorphic matrix multiplication."""
        # In real implementation, would work entirely in encrypted domain
        # This is a simplified simulation
        return encrypted_matrix @ weights


class AdvancedThreatDetector:
    """AI-powered threat detection for medical systems."""
    
    def __init__(self):
        self.threat_patterns = self._load_threat_patterns()
        self.anomaly_baseline = {}
        self.threat_scores = {}
        
    def detect_threats(self, request_data: Dict[str, Any]) -> Tuple[ThreatLevel, List[str]]:
        """Detect potential security threats in requests."""
        threats = []
        max_threat_level = ThreatLevel.LOW
        
        # Check for SQL injection attempts
        if self._detect_sql_injection(request_data):
            threats.append("SQL injection attempt detected")
            max_threat_level = max(max_threat_level, ThreatLevel.HIGH)
            
        # Check for unusual request patterns
        if self._detect_anomalous_behavior(request_data):
            threats.append("Anomalous request pattern detected")
            max_threat_level = max(max_threat_level, ThreatLevel.MEDIUM)
            
        # Check for data exfiltration attempts
        if self._detect_data_exfiltration(request_data):
            threats.append("Potential data exfiltration detected")
            max_threat_level = max(max_threat_level, ThreatLevel.CRITICAL)
            
        # Check for model extraction attacks
        if self._detect_model_extraction(request_data):
            threats.append("Model extraction attack detected")
            max_threat_level = max(max_threat_level, ThreatLevel.HIGH)
            
        # Check for adversarial inputs
        if self._detect_adversarial_input(request_data):
            threats.append("Adversarial input detected")
            max_threat_level = max(max_threat_level, ThreatLevel.MEDIUM)
            
        return max_threat_level, threats
        
    def _detect_sql_injection(self, request_data: Dict[str, Any]) -> bool:
        """Detect SQL injection attempts."""
        sql_patterns = [
            "union select", "drop table", "insert into", "delete from",
            "update set", "exec(", "script>", "javascript:",
            "../../", "../", "..\\", "..\\"
        ]
        
        request_str = json.dumps(request_data).lower()
        return any(pattern in request_str for pattern in sql_patterns)
        
    def _detect_anomalous_behavior(self, request_data: Dict[str, Any]) -> bool:
        """Detect anomalous request behavior."""
        # Check request size
        request_size = len(json.dumps(request_data))
        if request_size > 1000000:  # >1MB requests are suspicious
            return True
            
        # Check for unusual headers or parameters
        suspicious_keys = ["admin", "root", "system", "debug", "test"]
        request_str = json.dumps(request_data).lower()
        
        return any(key in request_str for key in suspicious_keys)
        
    def _detect_data_exfiltration(self, request_data: Dict[str, Any]) -> bool:
        """Detect potential data exfiltration attempts."""
        # Look for requests that might be trying to extract large amounts of data
        if "limit" in request_data:
            try:
                limit = int(request_data["limit"])
                if limit > 1000:  # Requesting more than 1000 records
                    return True
            except (ValueError, TypeError):
                pass
                
        # Check for bulk data request patterns
        bulk_patterns = ["export", "download", "backup", "dump", "extract"]
        request_str = json.dumps(request_data).lower()
        
        return any(pattern in request_str for pattern in bulk_patterns)
        
    def _detect_model_extraction(self, request_data: Dict[str, Any]) -> bool:
        """Detect model extraction attacks."""
        # Look for systematic probing patterns
        if "batch_size" in request_data:
            try:
                batch_size = int(request_data["batch_size"])
                if batch_size > 100:  # Large batch sizes might indicate extraction
                    return True
            except (ValueError, TypeError):
                pass
                
        # Check for model introspection attempts
        introspection_patterns = ["weights", "parameters", "layers", "architecture"]
        request_str = json.dumps(request_data).lower()
        
        return any(pattern in request_str for pattern in introspection_patterns)
        
    def _detect_adversarial_input(self, request_data: Dict[str, Any]) -> bool:
        """Detect adversarial inputs designed to fool the model."""
        # Check for unusual image properties (if image data is present)
        if "image_data" in request_data:
            # In real implementation, would analyze image for adversarial patterns
            # For now, just check for suspicious metadata
            if "noise_level" in request_data or "perturbation" in request_data:
                return True
                
        return False
        
    def _load_threat_patterns(self) -> Dict[str, List[str]]:
        """Load known threat patterns."""
        return {
            "malware_signatures": [
                "eval(", "exec(", "system(", "shell_exec",
                "passthru", "proc_open", "popen"
            ],
            "injection_patterns": [
                "union select", "drop table", "truncate table",
                "<script>", "javascript:", "vbscript:",
                "onload=", "onerror=", "onclick="
            ],
            "reconnaissance_patterns": [
                "information_schema", "sys.tables", "pg_tables",
                "show tables", "describe table", "explain plan"
            ]
        }


class SecurityAuditLogger:
    """Comprehensive security audit logging."""
    
    def __init__(self, log_file: str = "security_audit.log"):
        self.log_file = log_file
        self.logger = self._setup_logger()
        self._lock = threading.Lock()
        
    def _setup_logger(self) -> logging.Logger:
        """Setup secure audit logger."""
        logger = logging.getLogger("security_audit")
        logger.setLevel(logging.INFO)
        
        handler = logging.FileHandler(self.log_file, mode='a')
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S UTC'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
        
    def log_security_event(self, event: SecurityEvent) -> None:
        """Log a security event with proper formatting."""
        with self._lock:
            event_data = event.to_dict()
            self.logger.info(f"SECURITY_EVENT: {json.dumps(event_data)}")
            
            # Also log to console for critical events
            if event.severity in [ThreatLevel.HIGH, ThreatLevel.CRITICAL]:
                print(f"ðŸš¨ CRITICAL SECURITY EVENT: {event.event_type} - {event.details}")
                
    def log_access_attempt(
        self,
        user_id: str,
        resource: str,
        action: str,
        success: bool,
        source_ip: str = "",
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """Log access attempts for audit trail."""
        event = SecurityEvent(
            event_type="access_attempt",
            severity=ThreatLevel.LOW if success else ThreatLevel.MEDIUM,
            source_ip=source_ip,
            user_id=user_id,
            action=action,
            resource=resource,
            success=success,
            details=details or {}
        )
        self.log_security_event(event)
        
    def log_threat_detection(
        self,
        threat_level: ThreatLevel,
        threats: List[str],
        source_ip: str = "",
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """Log threat detection events."""
        event = SecurityEvent(
            event_type="threat_detection",
            severity=threat_level,
            source_ip=source_ip,
            action="threat_analysis",
            resource="inference_endpoint",
            success=False,
            details={
                "threats_detected": threats,
                **(details or {})
            }
        )
        self.log_security_event(event)


class QuantumSecurityFramework:
    """Main quantum-enhanced security framework."""
    
    def __init__(self):
        self.crypto = QuantumResistantCrypto()
        self.homomorphic = HomomorphicComputing()
        self.threat_detector = AdvancedThreatDetector()
        self.audit_logger = SecurityAuditLogger()
        
        # Security configuration
        self.security_policies = {
            SecurityLevel.PHI_PROTECTED: {
                "encryption_required": True,
                "quantum_safe": True,
                "audit_all_access": True,
                "differential_privacy": True
            },
            SecurityLevel.QUANTUM_SAFE: {
                "encryption_required": True,
                "quantum_safe": True,
                "homomorphic_computing": True,
                "advanced_threat_detection": True
            }
        }
        
    def secure_inference_request(
        self,
        request_data: Dict[str, Any],
        security_level: SecurityLevel = SecurityLevel.PHI_PROTECTED,
        user_id: str = "",
        source_ip: str = ""
    ) -> Tuple[bool, Dict[str, Any], List[str]]:
        """Secure an inference request with appropriate protection."""
        
        # Threat detection
        threat_level, threats = self.threat_detector.detect_threats(request_data)
        
        if threat_level in [ThreatLevel.HIGH, ThreatLevel.CRITICAL]:
            self.audit_logger.log_threat_detection(threat_level, threats, source_ip)
            return False, {}, threats
            
        # Apply security measures based on level
        policy = self.security_policies.get(security_level, {})
        secured_data = request_data.copy()
        
        if policy.get("encryption_required"):
            secured_data = self._encrypt_sensitive_data(secured_data, security_level)
            
        if policy.get("differential_privacy"):
            secured_data = self._apply_differential_privacy(secured_data)
            
        # Log access attempt
        self.audit_logger.log_access_attempt(
            user_id=user_id,
            resource="inference_endpoint",
            action="inference_request",
            success=True,
            source_ip=source_ip,
            details={"security_level": security_level.value}
        )
        
        return True, secured_data, []
        
    def encrypt_medical_data(
        self,
        data: Union[bytes, np.ndarray],
        security_level: SecurityLevel = SecurityLevel.PHI_PROTECTED
    ) -> Dict[str, Any]:
        """Encrypt medical data with appropriate protection level."""
        
        if isinstance(data, np.ndarray):
            data_bytes = data.tobytes()
            is_numpy = True
            shape = data.shape
            dtype = str(data.dtype)
        else:
            data_bytes = data
            is_numpy = False
            shape = None
            dtype = None
            
        if security_level == SecurityLevel.QUANTUM_SAFE:
            # Use quantum-resistant encryption
            public_key, private_key = self.crypto.generate_lattice_keypair()
            encrypted_data = self.crypto.encrypt_with_lattice(data_bytes, public_key)
            
            return {
                "encrypted_data": base64.b64encode(encrypted_data).decode(),
                "encryption_type": "lattice_based",
                "private_key": base64.b64encode(private_key).decode(),
                "is_numpy": is_numpy,
                "shape": shape,
                "dtype": dtype,
                "security_level": security_level.value
            }
        else:
            # Use AES-256 encryption
            key = self.crypto.generate_quantum_safe_key(256)
            encrypted_data = self.crypto.aes_encrypt(data_bytes, key)
            
            return {
                "encrypted_data": base64.b64encode(encrypted_data).decode(),
                "encryption_type": "aes_256",
                "key": base64.b64encode(key).decode(),
                "is_numpy": is_numpy,
                "shape": shape,
                "dtype": dtype,
                "security_level": security_level.value
            }
            
    def decrypt_medical_data(self, encrypted_package: Dict[str, Any]) -> Union[bytes, np.ndarray]:
        """Decrypt medical data."""
        encrypted_data = base64.b64decode(encrypted_package["encrypted_data"].encode())
        encryption_type = encrypted_package["encryption_type"]
        
        if encryption_type == "lattice_based":
            private_key = base64.b64decode(encrypted_package["private_key"].encode())
            decrypted_data = self.crypto.decrypt_with_lattice(encrypted_data, private_key)
        else:  # AES
            key = base64.b64decode(encrypted_package["key"].encode())
            decrypted_data = self.crypto.aes_decrypt(encrypted_data, key)
            
        # Reconstruct numpy array if needed
        if encrypted_package.get("is_numpy", False):
            return np.frombuffer(
                decrypted_data,
                dtype=encrypted_package["dtype"]
            ).reshape(encrypted_package["shape"])
        else:
            return decrypted_data
            
    def privacy_preserving_inference(
        self,
        data: np.ndarray,
        model_weights: np.ndarray,
        user_id: str = ""
    ) -> np.ndarray:
        """Perform privacy-preserving inference using homomorphic encryption."""
        
        # Encrypt data for homomorphic computation
        encrypted_data = self.homomorphic.encrypt_data_for_inference(data)
        
        # Perform inference on encrypted data
        encrypted_result = self.homomorphic.homomorphic_inference(encrypted_data, model_weights)
        
        # Decrypt result
        result = self.homomorphic.decrypt_inference_result(
            encrypted_result,
            encrypted_data["encryption_key"]
        )
        
        # Log privacy-preserving operation
        self.audit_logger.log_security_event(SecurityEvent(
            event_type="privacy_preserving_inference",
            severity=ThreatLevel.LOW,
            user_id=user_id,
            action="homomorphic_inference",
            resource="ml_model",
            success=True,
            details={"data_shape": data.shape}
        ))
        
        return result
        
    def _encrypt_sensitive_data(self, data: Dict[str, Any], security_level: SecurityLevel) -> Dict[str, Any]:
        """Encrypt sensitive fields in request data."""
        sensitive_fields = ["patient_id", "medical_data", "image_data", "personal_info"]
        
        encrypted_data = data.copy()
        for field in sensitive_fields:
            if field in encrypted_data:
                field_data = json.dumps(encrypted_data[field]).encode()
                encrypted_package = self.encrypt_medical_data(field_data, security_level)
                encrypted_data[f"{field}_encrypted"] = encrypted_package
                del encrypted_data[field]
                
        return encrypted_data
        
    def _apply_differential_privacy(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Apply differential privacy noise to numerical data."""
        privacy_data = data.copy()
        
        # Add noise to numerical fields to ensure differential privacy
        for key, value in privacy_data.items():
            if isinstance(value, (int, float)):
                # Add Laplace noise for differential privacy
                epsilon = 1.0  # Privacy parameter
                sensitivity = 1.0  # Data sensitivity
                noise_scale = sensitivity / epsilon
                
                noise = np.random.laplace(0, noise_scale)
                privacy_data[key] = value + noise
                
        return privacy_data


# Global security framework instance
security_framework = QuantumSecurityFramework()


def secure_medical_inference(security_level: SecurityLevel = SecurityLevel.PHI_PROTECTED):
    """Decorator for securing medical inference functions."""
    def decorator(func: Callable) -> Callable:
        def wrapper(*args, **kwargs):
            # Extract request context
            request_data = kwargs.get("request_data", {})
            user_id = kwargs.get("user_id", "")
            source_ip = kwargs.get("source_ip", "")
            
            # Apply security measures
            is_secure, secured_data, threats = security_framework.secure_inference_request(
                request_data, security_level, user_id, source_ip
            )
            
            if not is_secure:
                raise SecurityError(f"Security threats detected: {threats}")
                
            # Update kwargs with secured data
            kwargs["request_data"] = secured_data
            
            # Call original function
            result = func(*args, **kwargs)
            
            # Log successful inference
            security_framework.audit_logger.log_security_event(SecurityEvent(
                event_type="secure_inference_completed",
                severity=ThreatLevel.LOW,
                user_id=user_id,
                action="inference",
                resource="ml_model",
                success=True,
                details={"security_level": security_level.value}
            ))
            
            return result
            
        return wrapper
    return decorator


class SecurityError(Exception):
    """Security-related exception."""
    pass


if __name__ == "__main__":
    # Example usage and testing
    
    # Test quantum-resistant encryption
    framework = QuantumSecurityFramework()
    
    # Test medical data encryption
    medical_data = np.random.rand(100, 100).astype(np.float32)
    encrypted_package = framework.encrypt_medical_data(medical_data, SecurityLevel.QUANTUM_SAFE)
    decrypted_data = framework.decrypt_medical_data(encrypted_package)
    
    print(f"Original data shape: {medical_data.shape}")
    print(f"Decrypted data shape: {decrypted_data.shape}")
    print(f"Data integrity check: {np.allclose(medical_data, decrypted_data, rtol=1e-5)}")
    
    # Test privacy-preserving inference
    model_weights = np.random.rand(100, 10).astype(np.float32)
    result = framework.privacy_preserving_inference(medical_data, model_weights)
    print(f"Privacy-preserving inference result shape: {result.shape}")
    
    # Test threat detection
    suspicious_request = {
        "query": "union select * from patients",
        "limit": 10000,
        "admin": True
    }
    
    threat_level, threats = framework.threat_detector.detect_threats(suspicious_request)
    print(f"Threat level: {threat_level}")
    print(f"Threats detected: {threats}")
    
    print("Quantum security framework testing complete")